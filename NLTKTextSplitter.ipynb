{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN45ebrRX2p2X50D/Jgs0uw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidharth-157/Colab/blob/main/NLTKTextSplitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFDzCUHUoyEA",
        "outputId": "e707de33-1d12-4e6e-b294-aa329c808762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "nvSOxFn4o9Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yEKCHlclsyb",
        "outputId": "345611a2-eda7-48ac-d2ba-74dcd2407da3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "\n"
      ],
      "metadata": {
        "id": "G975a9RQo6RL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = NLTKTextSplitter(chunk_size=300)"
      ],
      "metadata": {
        "id": "B3euPBA3rB0p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_of_the_union = \"So far, we have explored two different families of generative models that have both involved latent variables—variational autoencoders (VAEs) and generative adversarial networks (GANs). In both cases, a new variable is introduced with a distribution that is easy to sample from and the model learns how to decode this variable back into the original domain. We will now turn our attention to autoregressive models—a family of models that simplify the generative modeling problem by treating it as a sequential process. Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable. Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs). In this chapter we shall explore two different autoregressive models: long short-term memory networks and PixelCNN. We will apply the LSTM to text data and the PixelCNN to image data. We will cover another highly successful autoregressive model, the Transformer, in detail in Foster, David. Generative Deep Learning (pp. 224-225). O'Reilly Media. Kindle Edition. So far, we have explored two different families of generative models that have both involved latent variables—variational autoencoders (VAEs) and generative adversarial networks (GANs). In both cases, a new variable is introduced with a distribution that is easy to sample from and the model learns how to decode this variable back into the original domain. We will now turn our attention to autoregressive models—a family of models that simplify the generative modeling problem by treating it as a sequential process. Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable. Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs). In this chapter we shall explore two different autoregressive models: long short-term memory networks and PixelCNN. We will apply the LSTM to text data and the PixelCNN to image data. We will cover another highly successful autoregressive model, the Transformer, in detail in Foster, David. Generative Deep Learning (pp. 224-225). O'Reilly Media. Kindle Edition. \""
      ],
      "metadata": {
        "id": "507tyDgQlFFZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "print(texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5rT0-V8peBP",
        "outputId": "d2fb0f5a-560b-484b-d662-7ca3c807a7ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So far, we have explored two different families of generative models that have both involved latent variables—variational autoencoders (VAEs) and generative adversarial networks (GANs).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts[1].split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m8AAceYprOk",
        "outputId": "76124eb6-3237-4c23-81cb-69cdec636615"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKtz92CJqXiC",
        "outputId": "0bcf4a63-2c24-4ce0-adf5-e9b84ee1c740"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texts)):\n",
        "  print(str(i)+\" \"+str(len(texts[i].split()))+\" \"+ texts[i])\n",
        "  print(\"================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3RCb2nQqMNK",
        "outputId": "878316f8-1ad7-47bd-ccab-ffec57002a5f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 24 So far, we have explored two different families of generative models that have both involved latent variables—variational autoencoders (VAEs) and generative adversarial networks (GANs).\n",
            "================================================\n",
            "1 31 In both cases, a new variable is introduced with a distribution that is easy to sample from and the model learns how to decode this variable back into the original domain.\n",
            "================================================\n",
            "2 42 We will now turn our attention to autoregressive models—a family of models that simplify the generative modeling problem by treating it as a sequential process.\n",
            "\n",
            "Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable.\n",
            "================================================\n",
            "3 38 Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable.\n",
            "\n",
            "Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs).\n",
            "================================================\n",
            "4 37 Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs).\n",
            "\n",
            "In this chapter we shall explore two different autoregressive models: long short-term memory networks and PixelCNN.\n",
            "================================================\n",
            "5 45 In this chapter we shall explore two different autoregressive models: long short-term memory networks and PixelCNN.\n",
            "\n",
            "We will apply the LSTM to text data and the PixelCNN to image data.\n",
            "\n",
            "We will cover another highly successful autoregressive model, the Transformer, in detail in Foster, David.\n",
            "================================================\n",
            "6 38 We will apply the LSTM to text data and the PixelCNN to image data.\n",
            "\n",
            "We will cover another highly successful autoregressive model, the Transformer, in detail in Foster, David.\n",
            "\n",
            "Generative Deep Learning (pp.\n",
            "\n",
            "224-225).\n",
            "\n",
            "O'Reilly Media.\n",
            "\n",
            "Kindle Edition.\n",
            "================================================\n",
            "7 33 Generative Deep Learning (pp.\n",
            "\n",
            "224-225).\n",
            "\n",
            "O'Reilly Media.\n",
            "\n",
            "Kindle Edition.\n",
            "\n",
            "So far, we have explored two different families of generative models that have both involved latent variables—variational autoencoders (VAEs) and generative adversarial networks (GANs).\n",
            "================================================\n",
            "8 31 In both cases, a new variable is introduced with a distribution that is easy to sample from and the model learns how to decode this variable back into the original domain.\n",
            "================================================\n",
            "9 42 We will now turn our attention to autoregressive models—a family of models that simplify the generative modeling problem by treating it as a sequential process.\n",
            "\n",
            "Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable.\n",
            "================================================\n",
            "10 38 Autoregressive models condition predictions on previous values in the sequence, rather than on a latent random variable.\n",
            "\n",
            "Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs).\n",
            "================================================\n",
            "11 37 Therefore, they attempt to explicitly model the data-generating distribution rather than an approximation of it (as in the case of VAEs).\n",
            "\n",
            "In this chapter we shall explore two different autoregressive models: long short-term memory networks and PixelCNN.\n",
            "================================================\n",
            "12 45 In this chapter we shall explore two different autoregressive models: long short-term memory networks and PixelCNN.\n",
            "\n",
            "We will apply the LSTM to text data and the PixelCNN to image data.\n",
            "\n",
            "We will cover another highly successful autoregressive model, the Transformer, in detail in Foster, David.\n",
            "================================================\n",
            "13 38 We will apply the LSTM to text data and the PixelCNN to image data.\n",
            "\n",
            "We will cover another highly successful autoregressive model, the Transformer, in detail in Foster, David.\n",
            "\n",
            "Generative Deep Learning (pp.\n",
            "\n",
            "224-225).\n",
            "\n",
            "O'Reilly Media.\n",
            "\n",
            "Kindle Edition.\n",
            "================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(state_of_the_union.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esnL36z-ps6o",
        "outputId": "b66a2bd0-0c90-4b62-8b66-f22f5f5925a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "344"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}